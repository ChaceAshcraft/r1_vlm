from transformers import AutoProcessor
from vllm import LLM, SamplingParams
from vllm.outputs import CompletionOutput, RequestOutput


def generate_completions_with_budget_forcing(vllm_inputs: list[dict], vlm: LLM, processor: AutoProcessor, max_thinking_tokens= 1024, num_ignore = 1, ignore_str="Wait"):
    '''
    Generate completions with budget forcing.
    
    vllm_inputs: A list of dictionaries, each containing a "prompt" key, which will be used as the prompts for the model. Generally created through the prepare_data method of an environment.
    vlm: The vlm model + vllm server to use for generation.
    processor: The processor to use for decoding token ids back to text.
    
    max_thinking_tokens: How many tokens we're willing to think for.
    num_ignore: How many times we're willing to ignore the model trying to end thinking.
    ignore_str: The string we manually add to promote the model to think more.
    '''
    
    # Need to output:
    # 1. completion.outputs[0].text - the text that represents the completion
    # 2. completion.prompt_token_ids - the token ids of the prompt
    # 3. completion.outputs[0].token_ids - the token ids of the completion
    
    
    # first, generate the model's initial response to the prompt. We will stop once either the model runs out of tokens
    # or the model attempts to end thinking.
    
    # track how many thinking tokens each element of the batch has used.
    thinking_tokens_used = [0 for _ in range(len(vllm_inputs))]
    
    stop_strs = [
        "</think>",
        "<answer>",
        "<tool>"
        "<|im_start|>",
        "<|im_end|>",
    ]
    
    sampling_params = SamplingParams(
        temperature=1.0,
        min_tokens=0,
        max_tokens=max_thinking_tokens,
        skip_special_tokens=False,
        # we interupt generation when the model attempts to end thinking, or start answering, or start using a tool.
        stop=stop_strs,
        # This should remove the stop string from the output. But I've found that it doesn't work when the stop string is multiple tokens.
        include_stop_str_in_output=False
    )
    
    outputs = vlm.generate(
        vllm_inputs,
        sampling_params=sampling_params,
    )
    
    # collect the prompt token ids for each element of the batch.
    # this is a list of lists of token ids. The length of the list is the same as vllm_inputs.
    prompt_token_ids = [output.prompt_token_ids for output in outputs]
    
    # collect the completion text generated by the model for each element of the batch.
    completion_texts = [output.outputs[0].text for output in outputs]
    
    # collect the token ids of the completion for each element of the batch.
    completion_token_ids = [output.outputs[0].token_ids for output in outputs]
    
    # the number of tokens used for each completion.
    completion_lengths = [len(completion_token_ids[i]) for i in range(len(completion_token_ids))]
    
    # update how many thinking tokens each element of the batch has used.
    for i in range(len(completion_lengths)):
        thinking_tokens_used[i] += completion_lengths[i]
        
    # update the batch prompts with the completion text.
    for i in range(len(completion_texts)):
        vllm_inputs[i]["prompt"] += completion_texts[i]
        
        
    # how many times we've forced the model to think more.
    num_ignores = 0
    
    # now we handle budget forcing. We continue generating as long as there is at least one element of the batch that has not used all of its thinking tokens
    # and we have not ignored the model too many times.
    while any([thinking_tokens_used[i] < max_thinking_tokens for i in range(len(thinking_tokens_used))]) and num_ignores < num_ignore:
        
        # determine which elements of the batch need to be forced to think more.
        indices_to_force_thinking = [i for i in range(len(thinking_tokens_used)) if thinking_tokens_used[i] < max_thinking_tokens]
        
        # TODO: Do the budget forcing batchwise. 
        # add the ignore string to all prompts that need more thinking
        for index in indices_to_force_thinking:
            vllm_inputs[index]["prompt"] += ignore_str
        
        # find the maximum number of remaining tokens across all inputs
        max_remaining_tokens = max([max_thinking_tokens - thinking_tokens_used[i] for i in indices_to_force_thinking])
        
        # keep only the inputs that need more thinking
        batch_vllm_inputs = [vllm_inputs[i].copy() for i in indices_to_force_thinking]
        
        sampling_params = SamplingParams(
            temperature=1.0,
            min_tokens=1,
            max_tokens=max_remaining_tokens,
            skip_special_tokens=False,
            stop=stop_strs,
            include_stop_str_in_output=False
        )
        
        # generate completions for all inputs in the batch at once
        batch_outputs = vlm.generate(
            batch_vllm_inputs,
            sampling_params=sampling_params,
        )
        
        # update each input with its completion, truncating if necessary
        for batch_idx, original_idx in enumerate(indices_to_force_thinking):
            # extract data from the output for this batch item
            completion_token_ids = batch_outputs[batch_idx].outputs[0].token_ids
            completion_text = batch_outputs[batch_idx].outputs[0].text
            
            # calculate the allowed remaining tokens for this input
            remaining_tokens = max_thinking_tokens - thinking_tokens_used[original_idx]
            
            # truncate token ids and text if they exceed the remaining tokens
            if len(completion_token_ids) > remaining_tokens:
                completion_token_ids = completion_token_ids[:remaining_tokens]
                # decode the truncated tokens to get the truncated text
                completion_text = processor.decode(completion_token_ids, skip_special_tokens=False, clean_up_tokenization_spaces=False)
            
            # update the number of tokens used
            completion_length = len(completion_token_ids)
            thinking_tokens_used[original_idx] += completion_length
            
            # update the element's prompt with the completion text
            vllm_inputs[original_idx]["prompt"] += completion_text
        
        num_ignores += 1
        print(f"Finished forcing thinking for {num_ignores} iterations.")
        print(f"Thinking tokens used: {thinking_tokens_used}")
    
    
    # at this point, all of the elements of the batch are done thinking, so we'll signal this and force it to return an answer. 
    for element in vllm_inputs:
        element["prompt"] += "</think>\n<answer>"
    
    sampling_params = SamplingParams(
        # TODO: is 100 tokens enough? It is right now...
        max_tokens=100,
        min_tokens=1,
        stop=[
                "<|im_end|>",
            ],
        )
    
    outputs = vlm.generate(
        vllm_inputs,
        sampling_params=sampling_params,
    )
    
    # collect the entire sequence of tokens for each element of the batch after generation. 
    all_ids = []
    for output in outputs:
        prompt_ids = output.prompt_token_ids
        completion_ids = output.outputs[0].token_ids
        all_ids.append(list(prompt_ids) + list(completion_ids))

    # now we'll use prompt_token_ids, a list of the prompt token ids from the first generation step to truncate all_ids.
    completion_ids = []
    for i in range(len(prompt_token_ids)):
        completion_ids.append(all_ids[i][len(prompt_token_ids[i]):])
    
    completion_texts = [processor.decode(completion_ids[i], skip_special_tokens=False, clean_up_tokenization_spaces=False) for i in range(len(completion_ids))]
    
    # repackage the data so it looks like it came right out of vllm. 
    request_outputs = []
    request_id = 0
    for prompt_token_ids_element, completion_ids, completion_text in zip(prompt_token_ids, completion_ids, completion_texts):
        request_outputs.append(RequestOutput(
            request_id=request_id,
            prompt_token_ids=prompt_token_ids_element,
            # the cumulative logprob and logprobs are dummy values for required fields.
            outputs=[CompletionOutput(index=0,text=completion_text, token_ids=completion_ids, cumulative_logprob = [], logprobs = [])],
            # extra dummy values for required fields.
            prompt="dummy placeholder", 
            prompt_logprobs = [], 
            finished=True,
        ))
        request_id += 1
    
    return request_outputs

