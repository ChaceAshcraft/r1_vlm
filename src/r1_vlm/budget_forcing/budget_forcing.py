

from transformers import AutoProcessor
from vllm import LLM, SamplingParams
from vllm.outputs import CompletionOutput, RequestOutput


def generate_completions_with_budget_forcing(vllm_inputs: list[dict], vlm: LLM, processor: AutoProcessor, max_thinking_tokens= 1024, num_ignore = 1, ignore_str="Wait"):
    '''
    Generate completions with budget forcing.
    
    vllm_inputs: A list of dictionaries, each containing a "prompt" key, which will be used as the prompts for the model. Generally created through the prepare_data method of an environment.
    vlm: The vlm model + vllm server to use for generation.
    
    max_thinking_tokens: How many tokens we're willing to think for.
    num_ignore: How many times we're willing to ignore the model trying to end thinking.
    ignore_str: The string we manually add to promote the model to think more.
    '''
    
    # Need to output:
    # 1. completion.outputs[0].text - the text that represents the completion
    # 2. completion.prompt_token_ids - the token ids of the prompt
    # 3. completion.outputs[0].token_ids - the token ids of the completion
    
    
    # first, generate the model's initial response to the prompt. We will stop once either the model runs out of tokens
    # or the model attempts to end thinking.
    
    # track how many thinking tokens each element of the batch has used.
    thinking_tokens_used = [0 for _ in range(len(vllm_inputs))]
    
    stop_strs = [
        "</think>",
        "<answer>",
        "<tool>"
        "<|im_start|>",
        "<|im_end|>",
    ]
    
    sampling_params = SamplingParams(
        temperature=1.0,
        min_tokens=0,
        max_tokens=max_thinking_tokens,
        skip_special_tokens=False,
        # we interupt generation when the model attempts to end thinking, or start answering, or start using a tool.
        stop=stop_strs,
        # This should remove the stop string from the output. But I've found that it doesn't work when the stop string is multiple tokens.
        include_stop_str_in_output=False
    )
    
    outputs = vlm.generate(
        vllm_inputs,
        sampling_params=sampling_params,
    )
    
    # collect the prompt token ids for each element of the batch.
    # this is a list of lists of token ids. The length of the list is the same as vllm_inputs.
    prompt_token_ids = [output.prompt_token_ids for output in outputs]
    
    # collect the completion text generated by the model for each element of the batch.
    completion_texts = [output.outputs[0].text for output in outputs]
    
    # collect the token ids of the completion for each element of the batch.
    completion_token_ids = [output.outputs[0].token_ids for output in outputs]
    
    # the number of tokens used for each completion.
    completion_lengths = [len(completion_token_ids[i]) for i in range(len(completion_token_ids))]
    
    # update how many thinking tokens each element of the batch has used.
    for i in range(len(completion_lengths)):
        thinking_tokens_used[i] += completion_lengths[i]
        
    # update the batch prompts with the completion text.
    for i in range(len(completion_texts)):
        vllm_inputs[i]["prompt"] += completion_texts[i]
        
        
    # how many times we've forced the model to think more.
    num_ignores = 0
    
    # now we handle budget forcing. We continue generating as long as there is at least one element of the batch that has not used all of its thinking tokens
    # and we have not ignored the model too many times.
    while any([thinking_tokens_used[i] < max_thinking_tokens for i in range(len(thinking_tokens_used))]) and num_ignores < num_ignore:
        
        # determine which elements of the batch need to be forced to think more.
        indices_to_force_thinking = [i for i in range(len(thinking_tokens_used)) if thinking_tokens_used[i] < max_thinking_tokens]
        
        # TODO: Do the budget forcing batchwise. 
        for index in indices_to_force_thinking:
            # add the ignore string to the prompt.
            vllm_inputs[index]["prompt"] += ignore_str
            
            remaining_tokens = max_thinking_tokens - thinking_tokens_used[index]
            vllm_input = [vllm_inputs[index].copy()]
            sampling_params = SamplingParams(
                temperature=1.0,
                min_tokens=1,
                max_tokens=remaining_tokens,
                skip_special_tokens=False,
                stop=stop_strs,
                include_stop_str_in_output=False
            )
            
            outputs = vlm.generate(
                vllm_input,
                sampling_params=sampling_params,
            )
            
            # extract data from the output. 
            completion_token_ids = outputs[0].outputs[0].token_ids
            completion_text = outputs[0].outputs[0].text
            completion_length = len(completion_token_ids)
            
            thinking_tokens_used[index] += completion_length
            
            # update the element's prompt with the completion text.
            vllm_inputs[index]["prompt"] += completion_text

        
        num_ignores += 1
        print(f"Finished forcing thinking for {num_ignores} iterations.")
        print(f"Thinking tokens used: {thinking_tokens_used}")
    
    
    # at this point, all of the elements of the batch are done thinking, so we'll signal this and force it to return an answer. 
    for element in vllm_inputs:
        element["prompt"] += "</think>\n<answer>"
    
    sampling_params = SamplingParams(
        # TODO: is 100 tokens enough? It is right now...
        max_tokens=100,
        min_tokens=1,
        stop=[
                "<|im_end|>",
            ],
        )
    
    outputs = vlm.generate(
        vllm_inputs,
        sampling_params=sampling_params,
    )
    
    # collect the entire sequence of tokens for each element of the batch after generation. 
    all_ids = []
    for output in outputs:
        prompt_ids = output.prompt_token_ids
        completion_ids = output.outputs[0].token_ids
        all_ids.append(list(prompt_ids) + list(completion_ids))

    # now we'll use prompt_token_ids, a list of the prompt token ids from the first generation step to truncate all_ids.
    completion_ids = []
    for i in range(len(prompt_token_ids)):
        completion_ids.append(all_ids[i][len(prompt_token_ids[i]):])
    
    completion_texts = [processor.decode(completion_ids[i], skip_special_tokens=False, clean_up_tokenization_spaces=False) for i in range(len(completion_ids))]
    
    # repackage the data so it looks like it came right out of vllm. 
    request_outputs = []
    request_id = 0
    for prompt_token_ids_element, completion_ids, completion_text in zip(prompt_token_ids, completion_ids, completion_texts):
        request_outputs.append(RequestOutput(
            request_id=request_id,
            prompt_token_ids=prompt_token_ids_element,
            # the cumulative logprob and logprobs are dummy values for required fields.
            outputs=[CompletionOutput(index=0,text=completion_text, token_ids=completion_ids, cumulative_logprob = [], logprobs = [])],
            # extra dummy values for required fields.
            prompt="dummy placeholder", 
            prompt_logprobs = [], 
            finished=True,
        ))
        request_id += 1
    
    return request_outputs

