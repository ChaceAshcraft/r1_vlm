
from unittest.mock import patch

from datasets import Dataset
from liger_kernel.transformers import apply_liger_kernel_to_qwen2_5_vl
from tqdm import tqdm
from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration
from vllm import LLM, SamplingParams

from r1_vlm.environments.real_iad_env.real_iad_simple_env import RealIADSimpleEnv
from trl import ModelConfig

# TODOs:
# 1. Do the budget forcing batchwise. 

def setup_model_and_processor() -> tuple[Qwen2_5_VLForConditionalGeneration, AutoProcessor]:
    '''
    Returns the model and processor.
    '''
    # this monkey patches the Qwen2.5-VL model to use the Liger Kernel on init. 
    apply_liger_kernel_to_qwen2_5_vl()

    model_name = "Qwen/Qwen2.5-VL-3B-Instruct"
    # model_name = "/millcreek/home/sunil/r1_vlm/vlm-r1-real-iad-simple-env/checkpoint-80"
    
    model_name = "Qwen/Qwen2.5-VL-3B-Instruct"
    model_config = ModelConfig(
        model_name_or_path=model_name,
        torch_dtype="bfloat16",
        use_peft=False,
    )

    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
        pretrained_model_name_or_path=model_config.model_name_or_path,
        torch_dtype=model_config.torch_dtype,
        use_cache=False,
    )
    model.eval()
    
    processor = AutoProcessor.from_pretrained(
        model_config.model_name_or_path, padding_side="left"
    )
    
    return model, processor


def setup_env(processor: AutoProcessor) -> RealIADSimpleEnv:
    vf_env = RealIADSimpleEnv(processing_class=processor)
    return vf_env

def dataset_to_batches(dataset: Dataset, batch_size: int) -> list[list[dict]]:
    '''
    Convert a dataset to a list of batches of examples.
    ''' 
    batches = []
    
    for example in dataset:
        if len(batches) == 0:
            batches.append([example])
        elif len(batches[-1]) < batch_size:
            batches[-1].append(example)
        else:
            batches.append([example])
        
    return batches

def setup_vllm(model: Qwen2_5_VLForConditionalGeneration):
    world_size_patch = patch("torch.distributed.get_world_size", return_value=1)
    profiling_patch = patch(
        "vllm.worker.worker.Worker._assert_memory_footprint_increased_during_profiling",
        return_value=None)
    
    with world_size_patch, profiling_patch:
        vlm = LLM(
            model=model.name_or_path,
            device="cuda:0",
            gpu_memory_utilization=1.0,
            dtype="bfloat16",
            enable_prefix_caching=True,
            limit_mm_per_prompt={"image": 1, "video": 0},
        )
    
    return vlm

def generate_completions(vllm_inputs, vlm, max_thinking_tokens= 1024, num_ignore = 1, ignore_str="Wait "):
    '''
    max_thinking_tokens: How many tokens we're willing to think for.
    num_ignore: How many times we're willing to ignore the model trying to end thinking.
    ignore_str: The string we manually add to promote the model to think more.
    '''
    
    # Need to output:
    # 1. completion.outputs[0].text - the next that represents the completion
    # 2. completion.prompt_token_ids - the token ids of the prompt
    # 3. completion.outputs[0].token_ids - the token ids of the completion
    
    
    # first, generate the model's initial response to the prompt. We will stop once either the model runs out of tokens
    # or the model attempts to end thinking.
    
    # track how many thinking tokens each element of the batch has used.
    thinking_tokens_used = [0 for _ in range(len(vllm_inputs))]
    
    stop_strs = [
        "</think>",
        "<answer>",
        "<tool>"
        "<|im_start|>",
        "<|im_end|>",
    ]
    
    sampling_params = SamplingParams(
        temperature=1.0,
        min_tokens=0,
        max_tokens=max_thinking_tokens,
        skip_special_tokens=False,
        # we interupt generation when the model attempts to end thinking, or start answering, or start using a tool.
        stop=stop_strs,
        # This should remove the stop string from the output. But I've found that it doesn't work when the stop string is multiple tokens.
        include_stop_str_in_output=False
    )
    
    outputs = vlm.generate(
        vllm_inputs,
        sampling_params=sampling_params,
    )
    
    # collect the prompt token ids for each element of the batch.
    # this is a list of lists of token ids. The length of the list is the same as vllm_inputs.
    prompt_token_ids = [output.prompt_token_ids for output in outputs]
    
    # collect the completion text generated by the model for each element of the batch.
    completion_texts = [output.outputs[0].text for output in outputs]
    
    # collect the token ids of the completion for each element of the batch.
    completion_token_ids = [output.outputs[0].token_ids for output in outputs]
    
    # the number of tokens used for each completion.
    completion_lengths = [len(completion_token_ids[i]) for i in range(len(completion_token_ids))]
    
    # update how many thinking tokens each element of the batch has used.
    for i in range(len(completion_lengths)):
        thinking_tokens_used[i] += completion_lengths[i]
        
    # update the batch prompts with the completion text.
    for i in range(len(completion_texts)):
        vllm_inputs[i]["prompt"] += completion_texts[i]
        
    
    # how many times we've forced the model to think more.
    num_ignores = 0
    
    # now we handle budget forcing. We continue generating as long as there is at least one element of the batch that has not used all of its thinking tokens
    # and we have not ignored the model too many times.
    while any([thinking_tokens_used[i] < max_thinking_tokens for i in range(len(thinking_tokens_used))]) and num_ignores < num_ignore:
        
        # determine which elements of the batch need to be forced to think more.
        indices_to_force_thinking = [i for i in range(len(thinking_tokens_used)) if thinking_tokens_used[i] < max_thinking_tokens]
        
        for index in indices_to_force_thinking:
            # add the ignore string to the prompt.
            vllm_inputs[index]["prompt"] += ignore_str
            
            remaining_tokens = max_thinking_tokens - thinking_tokens_used[index]
            vllm_input = [vllm_inputs[index].copy()]
            sampling_params = SamplingParams(
                temperature=1.0,
                min_tokens=1,
                max_tokens=remaining_tokens,
                skip_special_tokens=False,
                stop=stop_strs,
                include_stop_str_in_output=False
            )
            
            outputs = vlm.generate(
                vllm_input,
                sampling_params=sampling_params,
            )
            
            # extract data from the output. We don't need the prompt token ids as we already have them above.
            completion_token_ids = outputs[0].outputs[0].token_ids
            completion_text = outputs[0].outputs[0].text
            completion_length = len(completion_token_ids)
            
            thinking_tokens_used[index] += completion_length
            
            # update the element's prompt with the completion text.
            vllm_inputs[index]["prompt"] += completion_text
        
        num_ignores += 1
        print(f"Finished forcing thinking for {num_ignores} iterations.")
        print(f"Thinking tokens used: {thinking_tokens_used}")
    
        
        
            
            
            
            
    
    
    
    
    import ipdb; ipdb.set_trace()
    

    pass

if __name__ == "__main__":
    model, processor = setup_model_and_processor()
    env = setup_env(processor)
    _, test_dataset = env.get_dataset()
    
    # convert dataset to batches of examples
    batches = dataset_to_batches(test_dataset, 4)
    
    vlm = setup_vllm(model=model)
    
    for batch in tqdm(batches):
        conversations, texts, processed_batch, vllm_inputs = env.prepare_data(
            inputs=batch, processing_class=processor
        )
        
        completions = generate_completions(vllm_inputs=vllm_inputs, vlm=vlm)
